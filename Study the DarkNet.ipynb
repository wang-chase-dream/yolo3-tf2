{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.layers import (Add, BatchNormalization, Conv2D, LeakyReLU,\n",
    "                          ZeroPadding2D)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from utils.utils import compose\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0.0 Darknet.py\n",
    "\n",
    "For the param __padding__ of the Conv2D, one of \"valid\" or \"same\" (case-insensitive). \"valid\" means no padding. \"same\" results in padding with zeros evenly to the left/right or up/down of the input. __When padding=\"same\" and strides=1, the output has the same size as the input__ (This is the situation we used in the DarknetConv2D function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------#\n",
    "#   单次卷积\n",
    "#   DarknetConv2D\n",
    "#------------------------------------------------------#\n",
    "@wraps(Conv2D)\n",
    "def DarknetConv2D(*args, **kwargs):\n",
    "    # padding = \"valid\" means no padding, \n",
    "    darknet_conv_kwargs = {'kernel_initializer' : RandomNormal(stddev=0.02), 'kernel_regularizer': l2(5e-4)}\n",
    "    darknet_conv_kwargs['padding'] = 'valid' if kwargs.get('strides')==(2, 2) else 'same'\n",
    "    # The method update() adds dictionary dict2's key-values pairs in to dict.\n",
    "    darknet_conv_kwargs.update(kwargs)\n",
    "    return Conv2D(*args, **darknet_conv_kwargs)\n",
    "\n",
    "#---------------------------------------------------#\n",
    "#   卷积块 -> 卷积 + 标准化 + 激活函数\n",
    "#   DarknetConv2D + BatchNormalization + LeakyReLU\n",
    "#---------------------------------------------------#\n",
    "def DarknetConv2D_BN_Leaky(*args, **kwargs):\n",
    "    no_bias_kwargs = {'use_bias': False}\n",
    "    no_bias_kwargs.update(kwargs)\n",
    "    return compose( \n",
    "        DarknetConv2D(*args, **no_bias_kwargs),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.1))\n",
    "\n",
    "#---------------------------------------------------------------------#\n",
    "#   残差结构\n",
    "#   首先利用ZeroPadding2D和一个步长为2x2的卷积块进行高和宽的压缩\n",
    "#   然后对num_blocks进行循环，循环内部是残差结构。\n",
    "#---------------------------------------------------------------------#\n",
    "def resblock_body(x, num_filters, num_blocks):\n",
    "    x = ZeroPadding2D(((1,0),(1,0)))(x)\n",
    "    x = DarknetConv2D_BN_Leaky(num_filters, (3,3), strides=(2,2))(x)\n",
    "    for i in range(num_blocks):\n",
    "        y = DarknetConv2D_BN_Leaky(num_filters//2, (1,1))(x)\n",
    "        y = DarknetConv2D_BN_Leaky(num_filters, (3,3))(y)\n",
    "        x = Add()([x,y])\n",
    "    return x\n",
    "\n",
    "#---------------------------------------------------#\n",
    "#   darknet53 的主体部分\n",
    "#   输入为一张416x416x3的图片\n",
    "#   输出为三个有效特征层\n",
    "#---------------------------------------------------#\n",
    "def darknet_body(x):\n",
    "    # 416,416,3 -> 416,416,32\n",
    "    x = DarknetConv2D_BN_Leaky(32, (3,3))(x)\n",
    "    # 416,416,32 -> 208,208,64\n",
    "    x = resblock_body(x, 64, 1)\n",
    "    # 208,208,64 -> 104,104,128\n",
    "    x = resblock_body(x, 128, 2)\n",
    "    # 104,104,128 -> 52,52,256\n",
    "    x = resblock_body(x, 256, 8)\n",
    "    feat1 = x\n",
    "    # 52,52,256 -> 26,26,512\n",
    "    x = resblock_body(x, 512, 8)\n",
    "    feat2 = x\n",
    "    # 26,26,512 -> 13,13,1024\n",
    "    x = resblock_body(x, 1024, 4)\n",
    "    feat3 = x\n",
    "    return feat1, feat2, feat3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.0 My Own Example\n",
    "\n",
    "#### 1.1.1 Study of the DarknetConv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 414, 414, 32)\n",
      "(1, 416, 416, 32)\n"
     ]
    }
   ],
   "source": [
    "# Read the image\n",
    "path = \"img/street.jpg\"\n",
    "img = cv2.imread(path)\n",
    "# print(type(img)) <class 'numpy.ndarray'>\n",
    "new_img = cv2.resize(img, (416, 416))\n",
    "new_img = np.expand_dims(new_img, axis=0)\n",
    "new_img = new_img.astype(\"float\")\n",
    "# print(darknet_body(new_img))\n",
    "x = Conv2D(32, (3,3), padding=\"valid\")(new_img)\n",
    "y = DarknetConv2D(32, (3,3))(new_img)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Study of the resblock_body\n",
    "The __ZeroPadding2D__ layer can add rows and columns of zeros at the top, bottom, left and right side of an image tensor.\n",
    "\n",
    "For __ZeroPadding2D__, if __padding__ is tuple of 2 tuples of 2 ints: interpreted as ((top_pad, bottom_pad), (left_pad, right_pad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_img shape is: (1, 416, 416, 3)\n",
      "x shape is: (1, 417, 417, 3)\n",
      "y shape is: (1, 208, 208, 64)\n"
     ]
    }
   ],
   "source": [
    "# def resblock_body(x, num_filters, num_blocks):\n",
    "#     x = ZeroPadding2D(((1,0),(1,0)))(x)\n",
    "#     x = DarknetConv2D_BN_Leaky(num_filters, (3,3), strides=(2,2))(x)\n",
    "#     for i in range(num_blocks):\n",
    "#         y = DarknetConv2D_BN_Leaky(num_filters//2, (1,1))(x)\n",
    "#         y = DarknetConv2D_BN_Leaky(num_filters, (3,3))(y)\n",
    "#         x = Add()([x,y])\n",
    "#     return x\n",
    "\n",
    "print(\"new_img shape is:\", new_img.shape)\n",
    "x = ZeroPadding2D(((1,0),(1,0)))(new_img)\n",
    "y = DarknetConv2D_BN_Leaky(64, (3,3), strides=(2,2))(x)\n",
    "print(\"x shape is:\", x.shape)\n",
    "print(\"y shape is:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0.0 YoLo.py\n",
    "\n",
    "For __UpSampling2D__, tf.keras.layers.UpSampling2D(\n",
    "    size=(2, 2), data_format=None, interpolation='nearest', **kwargs\n",
    ").\n",
    "\n",
    "size: Int, or tuple of 2 integers. The upsampling factors for rows and columns.\n",
    "\n",
    "__Concatenate__: Layer that concatenates a list of inputs.\n",
    "\n",
    "tf.keras.layers.Concatenate(\n",
    "    axis=-1, **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate, Input, Lambda, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from utils.utils import compose\n",
    "\n",
    "from nets.darknet import DarknetConv2D, DarknetConv2D_BN_Leaky, darknet_body\n",
    "from nets.yolo_training import yolo_loss\n",
    "\n",
    "\n",
    "#---------------------------------------------------#\n",
    "#   特征层->最后的输出\n",
    "#---------------------------------------------------#\n",
    "def make_five_conv(x, num_filters):\n",
    "    x = DarknetConv2D_BN_Leaky(num_filters, (1,1))(x)\n",
    "    x = DarknetConv2D_BN_Leaky(num_filters*2, (3,3))(x)\n",
    "    x = DarknetConv2D_BN_Leaky(num_filters, (1,1))(x)\n",
    "    x = DarknetConv2D_BN_Leaky(num_filters*2, (3,3))(x)\n",
    "    x = DarknetConv2D_BN_Leaky(num_filters, (1,1))(x)\n",
    "    return x\n",
    "\n",
    "def make_yolo_head(x, num_filters, out_filters):\n",
    "    y = DarknetConv2D_BN_Leaky(num_filters*2, (3,3))(x)\n",
    "    # 255->3, 85->3, 4 + 1 + 80\n",
    "    y = DarknetConv2D(out_filters, (1,1))(y)\n",
    "    return y\n",
    "\n",
    "#---------------------------------------------------#\n",
    "#   FPN网络的构建，并且获得预测结果\n",
    "#---------------------------------------------------#\n",
    "def yolo_body(input_shape, anchors_mask, num_classes):\n",
    "    inputs      = Input(input_shape)\n",
    "    #---------------------------------------------------#   \n",
    "    #   生成darknet53的主干模型\n",
    "    #   获得三个有效特征层，他们的shape分别是：\n",
    "    #   C3 为 52,52,256\n",
    "    #   C4 为 26,26,512\n",
    "    #   C5 为 13,13,1024\n",
    "    #---------------------------------------------------#\n",
    "    C3, C4, C5  = darknet_body(inputs)\n",
    "\n",
    "    #---------------------------------------------------#\n",
    "    #   第一个特征层\n",
    "    #   y1=(batch_size,13,13,3,85)\n",
    "    #---------------------------------------------------#\n",
    "    # X: 13,13,1024 -> 13,13,512 -> 13,13,1024 -> 13,13,512 -> 13,13,1024 -> 13,13,512\n",
    "    x   = make_five_conv(C5, 512)\n",
    "    P5  = make_yolo_head(x, 512, len(anchors_mask[0]) * (num_classes+5))\n",
    "\n",
    "    # X: 13,13,512 -> 13,13,256 -> 26,26,256\n",
    "    x   = compose(DarknetConv2D_BN_Leaky(256, (1,1)), UpSampling2D(2))(x)\n",
    "\n",
    "    # X: 26,26,256 + 26,26,512 -> 26,26,768\n",
    "    x   = Concatenate()([x, C4])\n",
    "    #---------------------------------------------------#\n",
    "    #   第二个特征层\n",
    "    #   y2=(batch_size,26,26,3,85)\n",
    "    #---------------------------------------------------#\n",
    "    # X: 26,26,768 -> 26,26,256 -> 26,26,512 -> 26,26,256 -> 26,26,512 -> 26,26,256\n",
    "    x   = make_five_conv(x, 256)\n",
    "    P4  = make_yolo_head(x, 256, len(anchors_mask[1]) * (num_classes+5))\n",
    "\n",
    "    # X: 26,26,256 -> 26,26,128 -> 52,52,128\n",
    "    x   = compose(DarknetConv2D_BN_Leaky(128, (1,1)), UpSampling2D(2))(x)\n",
    "    # X: 52,52,128 + 52,52,256 -> 52,52,384\n",
    "    x   = Concatenate()([x, C3])\n",
    "    #---------------------------------------------------#\n",
    "    #   第三个特征层\n",
    "    #   y3=(batch_size,52,52,3,85)\n",
    "    #---------------------------------------------------#\n",
    "    # X: 52,52,384 -> 52,52,128 -> 52,52,256 -> 52,52,128 -> 52,52,256 -> 52,52,128\n",
    "    x   = make_five_conv(x, 128)\n",
    "    P3  = make_yolo_head(x, 128, len(anchors_mask[2]) * (num_classes+5))\n",
    "    return Model(inputs, [P5, P4, P3])\n",
    "\n",
    "\n",
    "def get_train_model(model_body, input_shape, num_classes, anchors, anchors_mask):\n",
    "    y_true = [Input(shape = (input_shape[0] // {0:32, 1:16, 2:8}[l], input_shape[1] // {0:32, 1:16, 2:8}[l], \\\n",
    "                                len(anchors_mask[l]), num_classes + 5)) for l in range(len(anchors_mask))]\n",
    "    # 看不懂\n",
    "    # arguments: Optional dictionary of keyword arguments to be passed to the function.\n",
    "    model_loss  = Lambda(\n",
    "        yolo_loss, \n",
    "        output_shape    = (1, ), \n",
    "        name            = 'yolo_loss', \n",
    "        arguments       = {'input_shape' : input_shape, 'anchors' : anchors, 'anchors_mask' : anchors_mask, 'num_classes' : num_classes}\n",
    "    )([*model_body.output, *y_true])\n",
    "    model       = Model([model_body.input, *y_true], model_loss)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.0 Research of the FPN\n",
    "\n",
    "Study of the Model() in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, None, None,  0           []                               \n",
      "                                 3)]                                                              \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)           [(None, None, None,  0           []                               \n",
      "                                 4)]                                                              \n",
      "                                                                                                  \n",
      " random_crop_1 (RandomCrop)     (None, 32, 32, 3)    0           ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# You can check the function API in tensorflow official documention to look more information\n",
    "inputs = keras.Input(shape=(None, None, 3))\n",
    "processed = keras.layers.RandomCrop(width=32, height=32)(inputs)\n",
    "conv = keras.layers.Conv2D(filters=2, kernel_size=3)(processed)\n",
    "pooling = keras.layers.GlobalAveragePooling2D()(conv)\n",
    "feature = keras.layers.Dense(10)(pooling)\n",
    "\n",
    "full_model = keras.Model(inputs, feature)\n",
    "backbone = keras.Model(processed, conv)\n",
    "activations = keras.Model(conv, feature)\n",
    "# full_model.summary()\n",
    "\n",
    "input2 = keras.Input(shape=(None, None, 4))\n",
    "test_model = keras.Model([inputs, input2], processed)\n",
    "test_model.summary()\n",
    "# backbone.summary()\n",
    "# activations.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0.0 train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from nets.yolo import get_train_model, yolo_body\n",
    "from utils.callbacks import (ExponentDecayScheduler, LossHistory,\n",
    "                             ModelCheckpoint)\n",
    "from utils.dataloader import YoloDatasets\n",
    "from utils.utils import get_anchors, get_classes\n",
    "from utils.utils_fit import fit_one_epoch\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    \n",
    "'''\n",
    "训练自己的目标检测模型一定需要注意以下几点：\n",
    "1、训练前仔细检查自己的格式是否满足要求，该库要求数据集格式为VOC格式，需要准备好的内容有输入图片和标签\n",
    "   输入图片为.jpg图片，无需固定大小，传入训练前会自动进行resize。\n",
    "   灰度图会自动转成RGB图片进行训练，无需自己修改。\n",
    "   输入图片如果后缀非jpg，需要自己批量转成jpg后再开始训练。\n",
    "\n",
    "   标签为.xml格式，文件中会有需要检测的目标信息，标签文件和输入图片文件相对应。\n",
    "\n",
    "2、训练好的权值文件保存在logs文件夹中，每个epoch都会保存一次，如果只是训练了几个step是不会保存的，epoch和step的概念要捋清楚一下。\n",
    "   在训练过程中，该代码并没有设定只保存最低损失的，因此按默认参数训练完会有100个权值，如果空间不够可以自行删除。\n",
    "   这个并不是保存越少越好也不是保存越多越好，有人想要都保存、有人想只保存一点，为了满足大多数的需求，还是都保存可选择性高。\n",
    "\n",
    "3、损失值的大小用于判断是否收敛，比较重要的是有收敛的趋势，即验证集损失不断下降，如果验证集损失基本上不改变的话，模型基本上就收敛了。\n",
    "   损失值的具体大小并没有什么意义，大和小只在于损失的计算方式，并不是接近于0才好。如果想要让损失好看点，可以直接到对应的损失函数里面除上10000。\n",
    "   训练过程中的损失值会保存在logs文件夹下的loss_%Y_%m_%d_%H_%M_%S文件夹中\n",
    "\n",
    "4、调参是一门蛮重要的学问，没有什么参数是一定好的，现有的参数是我测试过可以正常训练的参数，因此我会建议用现有的参数。\n",
    "   但是参数本身并不是绝对的，比如随着batch的增大学习率也可以增大，效果也会好一些；过深的网络不要用太大的学习率等等。\n",
    "   这些都是经验上，只能靠各位同学多查询资料和自己试试了。\n",
    "'''  \n",
    "if __name__ == \"__main__\":\n",
    "    #----------------------------------------------------#\n",
    "    #   是否使用eager模式训练\n",
    "    #----------------------------------------------------#\n",
    "    eager           = False\n",
    "    #--------------------------------------------------------#\n",
    "    #   训练前一定要修改classes_path，使其对应自己的数据集\n",
    "    #--------------------------------------------------------#\n",
    "    classes_path    = 'model_data/voc_classes.txt'\n",
    "    #---------------------------------------------------------------------#\n",
    "    #   anchors_path代表先验框对应的txt文件，一般不修改。\n",
    "    #   anchors_mask用于帮助代码找到对应的先验框，一般不修改。\n",
    "    #---------------------------------------------------------------------#\n",
    "    anchors_path    = 'model_data/yolo_anchors.txt'\n",
    "    anchors_mask    = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "    #----------------------------------------------------------------------------------------------------------------------------#\n",
    "    #   权值文件的下载请看README，可以通过网盘下载。模型的 预训练权重 对不同数据集是通用的，因为特征是通用的。\n",
    "    #   模型的 预训练权重 比较重要的部分是 主干特征提取网络的权值部分，用于进行特征提取。\n",
    "    #   预训练权重对于99%的情况都必须要用，不用的话主干部分的权值太过随机，特征提取效果不明显，网络训练的结果也不会好\n",
    "    #\n",
    "    #   如果训练过程中存在中断训练的操作，可以将model_path设置成logs文件夹下的权值文件，将已经训练了一部分的权值再次载入。\n",
    "    #   同时修改下方的 冻结阶段 或者 解冻阶段 的参数，来保证模型epoch的连续性。\n",
    "    #   \n",
    "    #   当model_path = ''的时候不加载整个模型的权值。\n",
    "    #\n",
    "    #   此处使用的是整个模型的权重，因此是在train.py进行加载的。\n",
    "    #   如果想要让模型从主干的预训练权值开始训练，则设置model_path为主干网络的权值，此时仅加载主干。\n",
    "    #   如果想要让模型从0开始训练，则设置model_path = ''，Freeze_Train = Fasle，此时从0开始训练，且没有冻结主干的过程。\n",
    "    #   一般来讲，从0开始训练效果会很差，因为权值太过随机，特征提取效果不明显。\n",
    "    #\n",
    "    #   网络一般不从0开始训练，至少会使用主干部分的权值，有些论文提到可以不用预训练，主要原因是他们 数据集较大 且 调参能力优秀。\n",
    "    #   如果一定要训练网络的主干部分，可以了解imagenet数据集，首先训练分类模型，分类模型的 主干部分 和该模型通用，基于此进行训练。\n",
    "    #----------------------------------------------------------------------------------------------------------------------------#\n",
    "    model_path      = 'model_data/yolo_weights.h5'\n",
    "    #------------------------------------------------------#\n",
    "    #   输入的shape大小，一定要是32的倍数\n",
    "    #------------------------------------------------------#\n",
    "    input_shape     = [416, 416]\n",
    "    \n",
    "    #----------------------------------------------------#\n",
    "    #   训练分为两个阶段，分别是冻结阶段和解冻阶段。\n",
    "    #   显存不足与数据集大小无关，提示显存不足请调小batch_size。\n",
    "    #   受到BatchNorm层影响，batch_size最小为2，不能为1。\n",
    "    #----------------------------------------------------#\n",
    "    #----------------------------------------------------#\n",
    "    #   冻结阶段训练参数\n",
    "    #   此时模型的主干被冻结了，特征提取网络不发生改变\n",
    "    #   占用的显存较小，仅对网络进行微调\n",
    "    #----------------------------------------------------#\n",
    "    Init_Epoch          = 0\n",
    "    Freeze_Epoch        = 50\n",
    "    Freeze_batch_size   = 8\n",
    "    Freeze_lr           = 1e-3\n",
    "    #----------------------------------------------------#\n",
    "    #   解冻阶段训练参数\n",
    "    #   此时模型的主干不被冻结了，特征提取网络会发生改变\n",
    "    #   占用的显存较大，网络所有的参数都会发生改变\n",
    "    #----------------------------------------------------#\n",
    "    UnFreeze_Epoch      = 100\n",
    "    Unfreeze_batch_size = 4\n",
    "    Unfreeze_lr         = 1e-4\n",
    "    #------------------------------------------------------#\n",
    "    #   是否进行冻结训练，默认先冻结主干训练后解冻训练。\n",
    "    #------------------------------------------------------#\n",
    "    Freeze_Train        = True\n",
    "    #------------------------------------------------------#\n",
    "    #   用于设置是否使用多线程读取数据，1代表关闭多线程\n",
    "    #   开启后会加快数据读取速度，但是会占用更多内存\n",
    "    #   keras里开启多线程有些时候速度反而慢了许多\n",
    "    #   在IO为瓶颈的时候再开启多线程，即GPU运算速度远大于读取图片的速度。\n",
    "    #   在eager模式为False有效\n",
    "    #------------------------------------------------------#\n",
    "    num_workers         = 1\n",
    "    #----------------------------------------------------#\n",
    "    #   获得图片路径和标签\n",
    "    #----------------------------------------------------#\n",
    "    train_annotation_path   = '2007_train.txt'\n",
    "    val_annotation_path     = '2007_val.txt'\n",
    "\n",
    "    #----------------------------------------------------#\n",
    "    #   获取classes和anchor\n",
    "    #----------------------------------------------------#\n",
    "    class_names, num_classes = get_classes(classes_path)\n",
    "    anchors, num_anchors     = get_anchors(anchors_path)\n",
    "\n",
    "    #------------------------------------------------------#\n",
    "    #   创建yolo模型\n",
    "    #------------------------------------------------------#\n",
    "    model_body  = yolo_body((None, None, 3), anchors_mask, num_classes)\n",
    "    if model_path != '':\n",
    "        #------------------------------------------------------#\n",
    "        #   载入预训练权重\n",
    "        #------------------------------------------------------#\n",
    "        print('Load weights {}.'.format(model_path))\n",
    "        model_body.load_weights(model_path, by_name=True, skip_mismatch=True)\n",
    "\n",
    "    if not eager:\n",
    "        model = get_train_model(model_body, input_shape, num_classes, anchors, anchors_mask)\n",
    "    #-------------------------------------------------------------------------------#\n",
    "    #   训练参数的设置\n",
    "    #   logging表示tensorboard的保存地址\n",
    "    #   checkpoint用于设置权值保存的细节，period用于修改多少epoch保存一次\n",
    "    #   reduce_lr用于设置学习率下降的方式\n",
    "    #   early_stopping用于设定早停，val_loss多次不下降自动结束训练，表示模型基本收敛\n",
    "    #-------------------------------------------------------------------------------#\n",
    "    logging         = TensorBoard(log_dir = 'logs/')\n",
    "    checkpoint      = ModelCheckpoint('logs/ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
    "                            monitor = 'val_loss', save_weights_only = True, save_best_only = False, period = 1)\n",
    "    reduce_lr       = ExponentDecayScheduler(decay_rate = 0.94, verbose = 1)\n",
    "    early_stopping  = EarlyStopping(monitor='val_loss', min_delta = 0, patience = 10, verbose = 1)\n",
    "    loss_history    = LossHistory('logs/')\n",
    "\n",
    "    #---------------------------#\n",
    "    #   读取数据集对应的txt\n",
    "    #---------------------------#\n",
    "    with open(train_annotation_path) as f:\n",
    "        train_lines = f.readlines()\n",
    "    with open(val_annotation_path) as f:\n",
    "        val_lines   = f.readlines()\n",
    "    num_train   = len(train_lines)\n",
    "    num_val     = len(val_lines)\n",
    "\n",
    "    if Freeze_Train:\n",
    "        freeze_layers = 184\n",
    "        for i in range(freeze_layers): model_body.layers[i].trainable = False\n",
    "        print('Freeze the first {} layers of total {} layers.'.format(freeze_layers, len(model_body.layers)))\n",
    "        \n",
    "    #------------------------------------------------------#\n",
    "    #   主干特征提取网络特征通用，冻结训练可以加快训练速度\n",
    "    #   也可以在训练初期防止权值被破坏。\n",
    "    #   Init_Epoch为起始世代\n",
    "    #   Freeze_Epoch为冻结训练的世代\n",
    "    #   UnFreeze_Epoch总训练世代\n",
    "    #   提示OOM或者显存不足请调小Batch_size\n",
    "    #------------------------------------------------------#\n",
    "    if True:\n",
    "        batch_size  = Freeze_batch_size\n",
    "        lr          = Freeze_lr\n",
    "        start_epoch = Init_Epoch\n",
    "        end_epoch   = Freeze_Epoch\n",
    "\n",
    "        epoch_step      = num_train // batch_size\n",
    "        epoch_step_val  = num_val   // batch_size\n",
    "\n",
    "        if epoch_step == 0 or epoch_step_val == 0:\n",
    "            raise ValueError('数据集过小，无法进行训练，请扩充数据集。')\n",
    "\n",
    "        train_dataloader    = YoloDatasets(train_lines, input_shape, anchors, batch_size, num_classes, anchors_mask, train = True)\n",
    "        val_dataloader      = YoloDatasets(val_lines, input_shape, anchors, batch_size, num_classes, anchors_mask, train = False)\n",
    "\n",
    "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "        if eager:\n",
    "            gen     = tf.data.Dataset.from_generator(partial(train_dataloader.generate), (tf.float32, tf.float32, tf.float32, tf.float32))\n",
    "            gen_val = tf.data.Dataset.from_generator(partial(val_dataloader.generate), (tf.float32, tf.float32, tf.float32, tf.float32))\n",
    "\n",
    "            gen     = gen.shuffle(buffer_size = batch_size).prefetch(buffer_size = batch_size)\n",
    "            gen_val = gen_val.shuffle(buffer_size = batch_size).prefetch(buffer_size = batch_size)\n",
    "\n",
    "            lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                initial_learning_rate = lr, decay_steps = epoch_step, decay_rate=0.94, staircase=True)\n",
    "            \n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate = lr_schedule)\n",
    "\n",
    "            for epoch in range(start_epoch, end_epoch):\n",
    "                fit_one_epoch(model_body, loss_history, optimizer, epoch, epoch_step, epoch_step_val, gen, gen_val, \n",
    "                            end_epoch, input_shape, anchors, anchors_mask, num_classes)\n",
    "\n",
    "        else:\n",
    "            model.compile(optimizer=Adam(lr = lr), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # 输出值即是loss value?\n",
    "\n",
    "            model.fit_generator(\n",
    "                generator           = train_dataloader,\n",
    "                steps_per_epoch     = epoch_step,\n",
    "                validation_data     = val_dataloader,\n",
    "                validation_steps    = epoch_step_val,\n",
    "                epochs              = end_epoch,\n",
    "                initial_epoch       = start_epoch,\n",
    "                use_multiprocessing = True if num_workers > 1 else False,\n",
    "                workers             = num_workers,\n",
    "                callbacks           = [logging, checkpoint, reduce_lr, early_stopping, loss_history]\n",
    "            )\n",
    "\n",
    "    if Freeze_Train:\n",
    "        for i in range(freeze_layers): model_body.layers[i].trainable = True\n",
    "\n",
    "    if True:\n",
    "        batch_size  = Unfreeze_batch_size\n",
    "        lr          = Unfreeze_lr\n",
    "        start_epoch = Freeze_Epoch\n",
    "        end_epoch   = UnFreeze_Epoch\n",
    "\n",
    "        epoch_step      = num_train // batch_size\n",
    "        epoch_step_val  = num_val   // batch_size\n",
    "\n",
    "        if epoch_step == 0 or epoch_step_val == 0:\n",
    "            raise ValueError('数据集过小，无法进行训练，请扩充数据集。')\n",
    "\n",
    "        train_dataloader    = YoloDatasets(train_lines, input_shape, anchors, batch_size, num_classes, anchors_mask, train = True)\n",
    "        val_dataloader      = YoloDatasets(val_lines, input_shape, anchors, batch_size, num_classes, anchors_mask, train = False)\n",
    "\n",
    "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "        if eager:\n",
    "            gen     = tf.data.Dataset.from_generator(partial(train_dataloader.generate), (tf.float32, tf.float32, tf.float32, tf.float32))\n",
    "            gen_val = tf.data.Dataset.from_generator(partial(val_dataloader.generate), (tf.float32, tf.float32, tf.float32, tf.float32))\n",
    "\n",
    "            gen     = gen.shuffle(buffer_size = batch_size).prefetch(buffer_size = batch_size)\n",
    "            gen_val = gen_val.shuffle(buffer_size = batch_size).prefetch(buffer_size = batch_size)\n",
    "\n",
    "            lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                initial_learning_rate = lr, decay_steps = epoch_step, decay_rate=0.94, staircase=True)\n",
    "            \n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate = lr_schedule)\n",
    "\n",
    "            for epoch in range(start_epoch, end_epoch):\n",
    "                fit_one_epoch(model_body, loss_history, optimizer, epoch, epoch_step, epoch_step_val, gen, gen_val, \n",
    "                            end_epoch, input_shape, anchors, anchors_mask, num_classes)\n",
    "\n",
    "        else:\n",
    "            # 看不懂\n",
    "            model.compile(optimizer=Adam(lr = lr), loss={'yolo_loss': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "            model.fit_generator(\n",
    "                generator           = train_dataloader,\n",
    "                steps_per_epoch     = epoch_step,\n",
    "                validation_data     = val_dataloader,\n",
    "                validation_steps    = epoch_step_val,\n",
    "                epochs              = end_epoch,\n",
    "                initial_epoch       = start_epoch,\n",
    "                use_multiprocessing = True if num_workers > 1 else False,\n",
    "                workers             = num_workers,\n",
    "                callbacks           = [logging, checkpoint, reduce_lr, early_stopping, loss_history]\n",
    "            )3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
